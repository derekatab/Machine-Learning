{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Feature Selection for Attrition / Burnout Prediction\n",
    "\n",
    "# Group 14: Rylie Ramos-Marquez, Derek Atabayev, Vishnu Garigipati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous assignment, we know that the mest method is gradient boosting of trees, specifically with 200 boosting rounds.\n",
    "\n",
    "Why it's the best:\n",
    "\n",
    "* High F1-score = 0.9319 which far surpasses other models (better by at least 0.1 = 10%), and is also better than simple KNN/Decision trees model\n",
    "\n",
    "* Shallow tree structure (max depth = 5) which prevents overfitting\n",
    "\n",
    "* Consistent performance with a tight 95% confidence interval of [0.855171, 0.893952] which is a very small range, meaning the performance is consistent and strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model is saved as a Pickle format for easy retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import joblib # since joblib.dump was used to save the model\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('scaler',\n",
      "                                                                   MinMaxScaler())]),\n",
      "                                                  Index(['hrs', 'absences', 'JobInvolvement', 'PerformanceRating',\n",
      "       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age',\n",
      "       'DistanceFromHome', 'Education', 'EmployeeID', 'JobLevel',\n",
      "       'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
      "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
      "       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier',\n",
      "                 GradientBoostingClassifier(max_depth=5, min_samples_split=5,\n",
      "                                            n_estimators=200,\n",
      "                                            random_state=100545358,\n",
      "                                            subsample=0.8))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ColumnTransformer from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DummyClassifier from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator GradientBoostingClassifier from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# retrieve final_model_14.pkl from ../final_model_14.pkl\n",
    "\n",
    "model = joblib.load('../final_model_14.pkl')\n",
    "\n",
    "# check if the model has been loaded correctly\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model has been loaded successfully. We can see that the feature names are correct and familiar, so the preprocessing and classifier are intact. All the gradient boosting parameters are there too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the grid search later, we will need our training data. We can pull that now too.\n",
    "\n",
    "We added a cell to the notebook from assignment 1 to export this data to pickle files\n",
    "\n",
    "Our ideal split from assignment one was 80/20 training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data X and y from pickle\n",
    "\n",
    "X = joblib.load('./X.pkl')\n",
    "y = joblib.load('./y.pkl')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100545358)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High correlation: PerformanceRating and PercentSalaryHike -> 0.79\n",
      "High correlation: PercentSalaryHike and PerformanceRating -> 0.79\n",
      "High correlation: YearsAtCompany and YearsWithCurrManager -> 0.76\n",
      "High correlation: YearsWithCurrManager and YearsAtCompany -> 0.76\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = X.corr(numeric_only=True)\n",
    "high_corr_pairs = correlation_matrix.where((correlation_matrix > 0.7) & (correlation_matrix < 1))\n",
    "\n",
    "for col in high_corr_pairs.columns:\n",
    "    high_corr_indices = high_corr_pairs.index[high_corr_pairs[col].notnull()]\n",
    "    for idx in high_corr_indices:\n",
    "        print(f\"High correlation: {col} and {idx} -> {correlation_matrix.loc[idx, col]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SelectKBest and criterion f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add feature selection with SelectKBest and f_classif to the pipeline\n",
    "\n",
    "# score_func is the function used to evaluate the importance of each feature\n",
    "# f_classif is a default value for the score_func parameter, calculates ANOVA F-Value\n",
    "\n",
    "# Extract components from the existing pipeline\n",
    "preprocessor = model.named_steps['preprocessor']\n",
    "classifier = model.named_steps['classifier']\n",
    "\n",
    "# create pipeline 1: feature selection with SelectKBest and criterion f_classif\n",
    "\n",
    "pipeline_f_classif = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest(f_classif)),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "pipeline_mutual_info_classif = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest(mutual_info_classif)),\n",
    "    ('classifier', classifier)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing grid search to tune the number of features to be selected (k parameter)\n",
    "\n",
    "  * We need to use an array to test different values of k, since we have 21 features, we can check 5, 10, 15, 20, 21 possibilities\n",
    "  * This will tell us whether the higher side or lower side is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of k for f_classif: 21\n",
      "Optimal value of k for mutual_info_classif: 21\n",
      "\n",
      "Average accuracy for each k (f_classif):\n",
      "k = 1: Average Accuracy = 0.6202\n",
      "k = 2: Average Accuracy = 0.6456\n",
      "k = 3: Average Accuracy = 0.6947\n",
      "k = 4: Average Accuracy = 0.7553\n",
      "k = 5: Average Accuracy = 0.7991\n",
      "k = 6: Average Accuracy = 0.8088\n",
      "k = 7: Average Accuracy = 0.8298\n",
      "k = 8: Average Accuracy = 0.8421\n",
      "k = 9: Average Accuracy = 0.8386\n",
      "k = 10: Average Accuracy = 0.8325\n",
      "k = 11: Average Accuracy = 0.8368\n",
      "k = 12: Average Accuracy = 0.8544\n",
      "k = 13: Average Accuracy = 0.8386\n",
      "k = 14: Average Accuracy = 0.8447\n",
      "k = 15: Average Accuracy = 0.8456\n",
      "k = 16: Average Accuracy = 0.8526\n",
      "k = 17: Average Accuracy = 0.8605\n",
      "k = 18: Average Accuracy = 0.8649\n",
      "k = 19: Average Accuracy = 0.8684\n",
      "k = 20: Average Accuracy = 0.8754\n",
      "k = 21: Average Accuracy = 0.8781\n",
      "\n",
      "Average accuracy for each k (mutual_info_classif):\n",
      "k = 1: Average Accuracy = 0.7395\n",
      "k = 2: Average Accuracy = 0.8281\n",
      "k = 3: Average Accuracy = 0.8377\n",
      "k = 4: Average Accuracy = 0.8456\n",
      "k = 5: Average Accuracy = 0.8526\n",
      "k = 6: Average Accuracy = 0.8728\n",
      "k = 7: Average Accuracy = 0.8316\n",
      "k = 8: Average Accuracy = 0.8605\n",
      "k = 9: Average Accuracy = 0.8386\n",
      "k = 10: Average Accuracy = 0.8386\n",
      "k = 11: Average Accuracy = 0.8316\n",
      "k = 12: Average Accuracy = 0.8561\n",
      "k = 13: Average Accuracy = 0.8377\n",
      "k = 14: Average Accuracy = 0.8535\n",
      "k = 15: Average Accuracy = 0.8561\n",
      "k = 16: Average Accuracy = 0.8623\n",
      "k = 17: Average Accuracy = 0.8754\n",
      "k = 18: Average Accuracy = 0.8605\n",
      "k = 19: Average Accuracy = 0.8667\n",
      "k = 20: Average Accuracy = 0.8658\n",
      "k = 21: Average Accuracy = 0.8781\n"
     ]
    }
   ],
   "source": [
    "# performing grid search to find the best parameters for the model, best metric is accuracy\n",
    "\n",
    "# define the parameters for the grid search\n",
    "\n",
    "# parameters for choosing a value of k\n",
    "param_grid = {\n",
    "    'selector__k': [i for i in range(1,22)]\n",
    "}\n",
    "\n",
    "def perform_grid_search(param_grid):\n",
    "    # create grid search object for pipeline 1\n",
    "    grid_search_f_classif = GridSearchCV(pipeline_f_classif, param_grid, cv=5, n_jobs=-1, verbose=0)\n",
    "    # in the previous assignment, we used 5-fold cross validation because the dataset is small, and we want to make sure that the model is not overfitting\n",
    "    # that approach worked well and so we can use it for HPO of the 'k' parameter too\n",
    "\n",
    "    # creating grid search object for pipeline 2\n",
    "    grid_search_mutual_info_classif = GridSearchCV(pipeline_mutual_info_classif, param_grid, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "    # fit the grid search objects to the data\n",
    "    grid_search_f_classif.fit(X_train, y_train_encoded)\n",
    "    grid_search_mutual_info_classif.fit(X_train, y_train_encoded)\n",
    "\n",
    "    # print the optimal k in both cases\n",
    "    best_k_f_classif = grid_search_f_classif.best_params_['selector__k']\n",
    "    print(f'Optimal value of k for f_classif: {best_k_f_classif}')\n",
    "\n",
    "    best_k_mutual_info_classif = grid_search_mutual_info_classif.best_params_['selector__k']\n",
    "    print(f'Optimal value of k for mutual_info_classif: {best_k_mutual_info_classif}')\n",
    "\n",
    "    print(\"\\nAverage accuracy for each k (f_classif):\")\n",
    "    for mean, params in zip(grid_search_f_classif.cv_results_['mean_test_score'], grid_search_f_classif.cv_results_['params']):\n",
    "        print(f\"k = {params['selector__k']}: Average Accuracy = {mean:.4f}\")\n",
    "\n",
    "    # Print average accuracy for each k for mutual_info_classif\n",
    "    print(\"\\nAverage accuracy for each k (mutual_info_classif):\")\n",
    "    for mean, params in zip(grid_search_mutual_info_classif.cv_results_['mean_test_score'], grid_search_mutual_info_classif.cv_results_['params']):\n",
    "        print(f\"k = {params['selector__k']}: Average Accuracy = {mean:.4f}\")\n",
    "    return grid_search_f_classif, grid_search_mutual_info_classif\n",
    "\n",
    "# Call the function with the parameter grid\n",
    "grid_search_f_classif, grid_search_mutual_info_classif = perform_grid_search(param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average optimal value of k for f_classif over 30 runs: 18.1\n",
      "Average optimal value of k for mutual_info_classif over 30 runs: 17.3\n",
      "\n",
      "Average accuracy for each k (f_classif):\n",
      "k = 1: Average Accuracy = 0.6047 ± 0.0128\n",
      "k = 2: Average Accuracy = 0.6594 ± 0.0136\n",
      "k = 3: Average Accuracy = 0.7223 ± 0.0126\n",
      "k = 4: Average Accuracy = 0.7791 ± 0.0099\n",
      "k = 5: Average Accuracy = 0.8052 ± 0.0085\n",
      "k = 6: Average Accuracy = 0.8312 ± 0.0101\n",
      "k = 7: Average Accuracy = 0.8487 ± 0.0092\n",
      "k = 8: Average Accuracy = 0.8558 ± 0.0090\n",
      "k = 9: Average Accuracy = 0.8635 ± 0.0082\n",
      "k = 10: Average Accuracy = 0.8634 ± 0.0089\n",
      "k = 11: Average Accuracy = 0.8624 ± 0.0102\n",
      "k = 12: Average Accuracy = 0.8638 ± 0.0091\n",
      "k = 13: Average Accuracy = 0.8632 ± 0.0110\n",
      "k = 14: Average Accuracy = 0.8651 ± 0.0085\n",
      "k = 15: Average Accuracy = 0.8668 ± 0.0091\n",
      "k = 16: Average Accuracy = 0.8664 ± 0.0106\n",
      "k = 17: Average Accuracy = 0.8692 ± 0.0116\n",
      "k = 18: Average Accuracy = 0.8687 ± 0.0086\n",
      "k = 19: Average Accuracy = 0.8724 ± 0.0068\n",
      "k = 20: Average Accuracy = 0.8735 ± 0.0074\n",
      "k = 21: Average Accuracy = 0.8748 ± 0.0083\n",
      "\n",
      "Average accuracy for each k (mutual_info_classif):\n",
      "k = 1: Average Accuracy = 0.7480 ± 0.0194\n",
      "k = 2: Average Accuracy = 0.8224 ± 0.0118\n",
      "k = 3: Average Accuracy = 0.8409 ± 0.0130\n",
      "k = 4: Average Accuracy = 0.8469 ± 0.0111\n",
      "k = 5: Average Accuracy = 0.8427 ± 0.0119\n",
      "k = 6: Average Accuracy = 0.8466 ± 0.0133\n",
      "k = 7: Average Accuracy = 0.8482 ± 0.0146\n",
      "k = 8: Average Accuracy = 0.8508 ± 0.0137\n",
      "k = 9: Average Accuracy = 0.8536 ± 0.0146\n",
      "k = 10: Average Accuracy = 0.8599 ± 0.0131\n",
      "k = 11: Average Accuracy = 0.8594 ± 0.0110\n",
      "k = 12: Average Accuracy = 0.8594 ± 0.0097\n",
      "k = 13: Average Accuracy = 0.8596 ± 0.0101\n",
      "k = 14: Average Accuracy = 0.8635 ± 0.0114\n",
      "k = 15: Average Accuracy = 0.8653 ± 0.0104\n",
      "k = 16: Average Accuracy = 0.8675 ± 0.0113\n",
      "k = 17: Average Accuracy = 0.8706 ± 0.0101\n",
      "k = 18: Average Accuracy = 0.8706 ± 0.0103\n",
      "k = 19: Average Accuracy = 0.8737 ± 0.0095\n",
      "k = 20: Average Accuracy = 0.8735 ± 0.0081\n",
      "k = 21: Average Accuracy = 0.8737 ± 0.0096\n",
      "\n",
      "\n",
      " best k_f [18, 11, 21, 17, 17, 20, 19, 19, 21, 14, 19, 20, 16, 19, 19, 19, 21, 20, 21, 15, 15, 20, 18, 21, 19, 17, 15, 17, 16, 19]\n",
      "\n",
      "\n",
      " best mutual info [19, 12, 18, 15, 19, 18, 18, 20, 19, 20, 19, 16, 18, 20, 19, 16, 13, 15, 11, 19, 20, 21, 10, 18, 20, 19, 19, 19, 12, 17]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def perform_multiple_grid_search(param_grid, X_train, y_train_encoded, n_runs=20):\n",
    "    best_ks_f_classif = []\n",
    "    best_ks_mutual_info_classif = []\n",
    "    \n",
    "    # Store accuracies for each k\n",
    "    accuracy_f_classif = {k: [] for k in param_grid['selector__k']}\n",
    "    accuracy_mutual_info_classif = {k: [] for k in param_grid['selector__k']}\n",
    "    \n",
    "    # Perform grid search over multiple runs\n",
    "    for _ in range(n_runs):\n",
    "        # Create cross-validation strategy\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
    "        \n",
    "        # Define grid search objects\n",
    "        grid_search_f_classif = GridSearchCV(pipeline_f_classif, param_grid, cv=cv, n_jobs=-1, verbose=0)\n",
    "        grid_search_mutual_info_classif = GridSearchCV(pipeline_mutual_info_classif, param_grid, cv=cv, n_jobs=-1, verbose=0)\n",
    "\n",
    "        # Fit the grid search objects to the data\n",
    "        grid_search_f_classif.fit(X_train, y_train_encoded)\n",
    "        grid_search_mutual_info_classif.fit(X_train, y_train_encoded)\n",
    "\n",
    "        # Get optimal k for each method\n",
    "        best_k_f_classif = grid_search_f_classif.best_params_['selector__k']\n",
    "        best_k_mutual_info_classif = grid_search_mutual_info_classif.best_params_['selector__k']\n",
    "        \n",
    "        best_ks_f_classif.append(best_k_f_classif)\n",
    "        best_ks_mutual_info_classif.append(best_k_mutual_info_classif)\n",
    "\n",
    "        # Record accuracies for each k\n",
    "        for mean, params in zip(grid_search_f_classif.cv_results_['mean_test_score'], grid_search_f_classif.cv_results_['params']):\n",
    "            accuracy_f_classif[params['selector__k']].append(mean)\n",
    "\n",
    "        for mean, params in zip(grid_search_mutual_info_classif.cv_results_['mean_test_score'], grid_search_mutual_info_classif.cv_results_['params']):\n",
    "            accuracy_mutual_info_classif[params['selector__k']].append(mean)\n",
    "    \n",
    "    # Calculate average optimal k and accuracy\n",
    "    avg_best_k_f_classif = np.mean(best_ks_f_classif)\n",
    "    avg_best_k_mutual_info_classif = np.mean(best_ks_mutual_info_classif)\n",
    "\n",
    "    print(f'Average optimal value of k for f_classif over {n_runs} runs: {avg_best_k_f_classif}')\n",
    "    print(f'Average optimal value of k for mutual_info_classif over {n_runs} runs: {avg_best_k_mutual_info_classif}')\n",
    "    \n",
    "    print(\"\\nAverage accuracy for each k (f_classif):\")\n",
    "    for k, accuracies in accuracy_f_classif.items():\n",
    "        print(f\"k = {k}: Average Accuracy = {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "    \n",
    "    print(\"\\nAverage accuracy for each k (mutual_info_classif):\")\n",
    "    for k, accuracies in accuracy_mutual_info_classif.items():\n",
    "        print(f\"k = {k}: Average Accuracy = {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "    print(f'\\n\\n best k_f {best_ks_f_classif}')\n",
    "    print(f'\\n\\n best mutual info {best_ks_mutual_info_classif}')\n",
    "    return avg_best_k_f_classif, avg_best_k_mutual_info_classif, accuracy_f_classif, accuracy_mutual_info_classif\n",
    "\n",
    "# Example usage:\n",
    "param_grid = {'selector__k': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}\n",
    "avg_best_k_f_classif, avg_best_k_mutual_info_classif, accuracy_f_classif, accuracy_mutual_info_classif = perform_multiple_grid_search(param_grid, X_train, y_train_encoded, n_runs=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the best k value is 20 in both cases; only one k-value is being discarded for minimal predictive power.\n",
    "\n",
    "This suggests that overall, the features havevery strong predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in list1: 19: 8, 21: 5, 17: 4, 20: 4, 15: 3, 18: 2, 16: 2, 11: 1, 14: 1\n",
      "Counts in list2: 19: 9, 18: 5, 20: 5, 12: 2, 15: 2, 16: 2, 13: 1, 11: 1, 21: 1, 10: 1, 17: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get the union of keys from both counters\n",
    "\n",
    "best_ks_f_classif = [18, 11, 21, 17, 17, 20, 19, 19, 21, 14, 19, 20, 16, 19, 19, 19, 21, 20, 21, 15, 15, 20, 18, 21, 19, 17, 15, 17, 16, 19]\n",
    "best_ks_mutual_info_classif = [19, 12, 18, 15, 19, 18, 18, 20, 19, 20, 19, 16, 18, 20, 19, 16, 13, 15, 11, 19, 20, 21, 10, 18, 20, 19, 19, 19, 12, 17]\n",
    "counter1 = Counter(best_ks_f_classif)\n",
    "counter2 = Counter(best_ks_mutual_info_classif)\n",
    "\n",
    "# Format counts and sort by values (counts) in descending order\n",
    "formatted_counts1 = \", \".join(f\"{key}: {value}\" for key, value in sorted(counter1.items(), key=lambda x: x[1], reverse=True))\n",
    "formatted_counts2 = \", \".join(f\"{key}: {value}\" for key, value in sorted(counter2.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Print results\n",
    "print(f\"Counts in list1: {formatted_counts1}\")\n",
    "print(f\"Counts in list2: {formatted_counts2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model with f_classif: 0.9265734265734266\n",
      "F1-score of the best model with f_classif: 0.9292929292929293\n",
      "Accuracy of the best model with mutual_info_classif: 0.9265734265734266\n",
      "F1-score of the best model with mutual_info_classif: 0.9292929292929293\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models obtained with the two pipelines on the testing dataset\n",
    "\n",
    "# get the best models from the grid search object\n",
    "best_model_f_classif = grid_search_f_classif.best_estimator_\n",
    "\n",
    "best_model_mutual_info_classif = grid_search_mutual_info_classif.best_estimator_\n",
    "\n",
    "# print the accuracy and f1-score of the best models on the testing dataset\n",
    "\n",
    "y_pred_f_classif = best_model_f_classif.predict(X_test)\n",
    "y_pred_mutual_info_classif = best_model_mutual_info_classif.predict(X_test)\n",
    "\n",
    "accuracy_f_classif = accuracy_score(y_test_encoded, y_pred_f_classif)\n",
    "f1_f_classif = f1_score(y_test_encoded, y_pred_f_classif)\n",
    "\n",
    "accuracy_mutual_info_classif = accuracy_score(y_test_encoded, y_pred_mutual_info_classif)\n",
    "f1_mutual_info_classif = f1_score(y_test_encoded, y_pred_mutual_info_classif)\n",
    "\n",
    "print(f'Accuracy of the best model with f_classif: {accuracy_f_classif}')\n",
    "print(f'F1-score of the best model with f_classif: {f1_f_classif}')\n",
    "\n",
    "print(f'Accuracy of the best model with mutual_info_classif: {accuracy_mutual_info_classif}')\n",
    "print(f'F1-score of the best model with mutual_info_classif: {f1_mutual_info_classif}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best feature selection method is with the criterion: mutual_info_classif\n",
    "\n",
    "The accuracy and f1-scores in this case are slightly better, but both models are pretty much the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dropped feature is: []\n"
     ]
    }
   ],
   "source": [
    "# Check which features are actually selected\n",
    "# We have 21, and k=20, so one has been dropped\n",
    "\n",
    "all_features = ['hrs', 'absences', 'JobInvolvement', 'PerformanceRating',\n",
    "       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age',\n",
    "       'DistanceFromHome', 'Education', 'EmployeeID', 'JobLevel',\n",
    "       'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
    "       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
    "# get the selected features from the best model with mutual_info_classif\n",
    "selected_features_mutual_info_classif = best_model_mutual_info_classif.named_steps['selector'].get_support()\n",
    "\n",
    "# find the feature that has been dropped\n",
    "dropped_feature = [feature for feature, selected in zip(all_features, selected_features_mutual_info_classif) if not selected] # zip is used to iterate over two lists at the same time\n",
    "print(f'The dropped feature is: {dropped_feature}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features, besides 'absences' are selected. 20 total features\n",
    "\n",
    "The results are improved as compared to the previous assignment, F1 Score has increased from 0.9320 to 0.9424\n",
    "\n",
    "Reason is likely that 'absences' might have been so poorly correlated to attrition that it contributed noise, which means the model can improve its predictions after removing it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the pipeline with k = 20\n",
    "\n",
    "pipeline_mutual_info_classif = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest(mutual_info_classif, k=20)), # add this feature selection\n",
    "    ('classifier', classifier)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
