{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Feature Selection for Attrition / Burnout Prediction\n",
    "\n",
    "# Group 14: Rylie Ramos-Marquez, Derek Atabayev, Vishnu Garigipati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous assignment, we know that the best method is gradient boosting of trees, specifically with 200 boosting rounds.\n",
    "\n",
    "Why it's the best:\n",
    "\n",
    "* High F1-score = 0.9319 which far surpasses other models (better by at least 0.1 = 10%), and is also better than simple KNN/Decision trees model\n",
    "\n",
    "* Shallow tree structure (max depth = 5) which prevents overfitting\n",
    "\n",
    "* Consistent performance with a tight 95% confidence interval of [0.855171, 0.893952] which is a very small range, meaning the performance is consistent and strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model is saved as a Pickle format for easy retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import joblib # since joblib.dump was used to save the model\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('scaler',\n",
      "                                                                   MinMaxScaler())]),\n",
      "                                                  Index(['hrs', 'absences', 'JobInvolvement', 'PerformanceRating',\n",
      "       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age',\n",
      "       'DistanceFromHome', 'Education', 'EmployeeID', 'JobLevel',\n",
      "       'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
      "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
      "       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier',\n",
      "                 GradientBoostingClassifier(max_depth=5, min_samples_split=5,\n",
      "                                            n_estimators=200,\n",
      "                                            random_state=100545358,\n",
      "                                            subsample=0.8))])\n"
     ]
    }
   ],
   "source": [
    "# retrieve final_model_14.pkl from ../final_model_14.pkl\n",
    "\n",
    "model = joblib.load('../final_model_14.pkl')\n",
    "\n",
    "# check if the model has been loaded correctly\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model has been loaded successfully. We can see that the feature names are correct and familiar, so the preprocessing and classifier are intact. All the gradient boosting parameters are there too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the grid search later, we will need our training data. We can pull that now too.\n",
    "\n",
    "We added a cell to the notebook from assignment 1 to export this data to pickle files\n",
    "\n",
    "Our ideal split from assignment one was 80/20 training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data X and y from pickle\n",
    "\n",
    "X = joblib.load('./X.pkl')\n",
    "y = joblib.load('./y.pkl')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100545358)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which pairs of indices have a high correlation.\n",
    "\n",
    "A correlation score close to one would suggest that through feature selection we can remove one element of the pair (since they are contributing similar predictive power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High correlation: PerformanceRating and PercentSalaryHike -> 0.79\n",
      "High correlation: PercentSalaryHike and PerformanceRating -> 0.79\n",
      "High correlation: YearsAtCompany and YearsWithCurrManager -> 0.76\n",
      "High correlation: YearsWithCurrManager and YearsAtCompany -> 0.76\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = X.corr(numeric_only=True)\n",
    "high_corr_pairs = correlation_matrix.where((correlation_matrix > 0.7) & (correlation_matrix < 1))\n",
    "\n",
    "for col in high_corr_pairs.columns:\n",
    "    high_corr_indices = high_corr_pairs.index[high_corr_pairs[col].notnull()]\n",
    "    for idx in high_corr_indices:\n",
    "        print(f\"High correlation: {col} and {idx} -> {correlation_matrix.loc[idx, col]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SelectKBest and criterion f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add feature selection with SelectKBest and f_classif to the pipeline\n",
    "\n",
    "# score_func is the function used to evaluate the importance of each feature\n",
    "# f_classif is a default value for the score_func parameter, calculates ANOVA F-Value\n",
    "\n",
    "# Extract components from the existing pipeline\n",
    "preprocessor = model.named_steps['preprocessor']\n",
    "classifier = model.named_steps['classifier']\n",
    "\n",
    "# create pipeline 1: feature selection with SelectKBest and criterion f_classif\n",
    "\n",
    "pipeline_f_classif = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest(f_classif)),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "pipeline_mutual_info_classif = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest(mutual_info_classif)),\n",
    "    ('classifier', classifier)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We place the selector function in the pieline after preprocessing (encoding the values) but before it is classified (through the gradient boosting model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing grid search to tune the number of features to be selected (k parameter)\n",
    "\n",
    "  * We need to use an array to test different values of k, since we have 21 features, we can check check all of them through an array builder (loop through 1 to 21)\n",
    "  * This will tell us whether the higher side or lower side is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of k for f_classif: 19\n",
      "Optimal value of k for mutual_info_classif: 17\n",
      "\n",
      "Average accuracy for each k (f_classif):\n",
      "k = 1: Average Accuracy = 0.6202\n",
      "k = 2: Average Accuracy = 0.6526\n",
      "k = 3: Average Accuracy = 0.6921\n",
      "k = 4: Average Accuracy = 0.7719\n",
      "k = 5: Average Accuracy = 0.7886\n",
      "k = 6: Average Accuracy = 0.8132\n",
      "k = 7: Average Accuracy = 0.8246\n",
      "k = 8: Average Accuracy = 0.8333\n",
      "k = 9: Average Accuracy = 0.8368\n",
      "k = 10: Average Accuracy = 0.8342\n",
      "k = 11: Average Accuracy = 0.8325\n",
      "k = 12: Average Accuracy = 0.8456\n",
      "k = 13: Average Accuracy = 0.8535\n",
      "k = 14: Average Accuracy = 0.8421\n",
      "k = 15: Average Accuracy = 0.8579\n",
      "k = 16: Average Accuracy = 0.8526\n",
      "k = 17: Average Accuracy = 0.8544\n",
      "k = 18: Average Accuracy = 0.8640\n",
      "k = 19: Average Accuracy = 0.8763\n",
      "k = 20: Average Accuracy = 0.8763\n",
      "k = 21: Average Accuracy = 0.8746\n",
      "\n",
      "Average accuracy for each k (mutual_info_classif):\n",
      "k = 1: Average Accuracy = 0.7693\n",
      "k = 2: Average Accuracy = 0.8316\n",
      "k = 3: Average Accuracy = 0.8412\n",
      "k = 4: Average Accuracy = 0.8447\n",
      "k = 5: Average Accuracy = 0.8561\n",
      "k = 6: Average Accuracy = 0.8351\n",
      "k = 7: Average Accuracy = 0.8325\n",
      "k = 8: Average Accuracy = 0.8325\n",
      "k = 9: Average Accuracy = 0.8474\n",
      "k = 10: Average Accuracy = 0.8561\n",
      "k = 11: Average Accuracy = 0.8737\n",
      "k = 12: Average Accuracy = 0.8500\n",
      "k = 13: Average Accuracy = 0.8482\n",
      "k = 14: Average Accuracy = 0.8702\n",
      "k = 15: Average Accuracy = 0.8412\n",
      "k = 16: Average Accuracy = 0.8588\n",
      "k = 17: Average Accuracy = 0.8746\n",
      "k = 18: Average Accuracy = 0.8518\n",
      "k = 19: Average Accuracy = 0.8711\n",
      "k = 20: Average Accuracy = 0.8684\n",
      "k = 21: Average Accuracy = 0.8746\n"
     ]
    }
   ],
   "source": [
    "# performing grid search to find the best parameters for the model, best metric is accuracy\n",
    "\n",
    "# define the parameters for the grid search\n",
    "\n",
    "# parameters for choosing a value of k\n",
    "param_grid = {\n",
    "    'selector__k': [i for i in range(1,22)]\n",
    "}\n",
    "\n",
    "def perform_grid_search(param_grid):\n",
    "    # create grid search object for pipeline 1\n",
    "    grid_search_f_classif = GridSearchCV(pipeline_f_classif, param_grid, cv=5, n_jobs=-1, verbose=0)\n",
    "    # in the previous assignment, we used 5-fold cross validation because the dataset is small, and we want to make sure that the model is not overfitting\n",
    "    # that approach worked well and so we can use it for HPO of the 'k' parameter too\n",
    "\n",
    "    # creating grid search object for pipeline 2\n",
    "    grid_search_mutual_info_classif = GridSearchCV(pipeline_mutual_info_classif, param_grid, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "    # fit the grid search objects to the data\n",
    "    grid_search_f_classif.fit(X_train, y_train_encoded)\n",
    "    grid_search_mutual_info_classif.fit(X_train, y_train_encoded)\n",
    "\n",
    "    # print the optimal k in both cases\n",
    "    best_k_f_classif = grid_search_f_classif.best_params_['selector__k']\n",
    "    print(f'Optimal value of k for f_classif: {best_k_f_classif}')\n",
    "\n",
    "    best_k_mutual_info_classif = grid_search_mutual_info_classif.best_params_['selector__k']\n",
    "    print(f'Optimal value of k for mutual_info_classif: {best_k_mutual_info_classif}')\n",
    "\n",
    "    print(\"\\nAverage accuracy for each k (f_classif):\")\n",
    "    for mean, params in zip(grid_search_f_classif.cv_results_['mean_test_score'], grid_search_f_classif.cv_results_['params']):\n",
    "        print(f\"k = {params['selector__k']}: Average Accuracy = {mean:.4f}\")\n",
    "\n",
    "    # Print average accuracy for each k for mutual_info_classif\n",
    "    print(\"\\nAverage accuracy for each k (mutual_info_classif):\")\n",
    "    for mean, params in zip(grid_search_mutual_info_classif.cv_results_['mean_test_score'], grid_search_mutual_info_classif.cv_results_['params']):\n",
    "        print(f\"k = {params['selector__k']}: Average Accuracy = {mean:.4f}\")\n",
    "    return grid_search_f_classif, grid_search_mutual_info_classif\n",
    "\n",
    "# Call the function with the parameter grid\n",
    "grid_search_f_classif, grid_search_mutual_info_classif = perform_grid_search(param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal 'k' value for each feature selection is going to different across many trials.\n",
    "\n",
    "Reasons: \n",
    "\n",
    "* The data may be shuffled differently, which leads to variation in splits and a change in performance for different k.\n",
    "\n",
    "* Since the dataset is small, the splitting can affect the k\n",
    "\n",
    "* Since it is a small dataset, the accuracy difference is small, but can still result in a different recommendation \n",
    "\n",
    " * see the output for the accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can average across many trials to find the true \"optimal\" k value\n",
    "\n",
    "Perform 30 runs and take the average (long execution time, but optimized results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     58\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselector__k\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m21\u001b[39m]}\n\u001b[1;32m---> 59\u001b[0m avg_best_k_f_classif, avg_best_k_mutual_info_classif, accuracy_f_classif, accuracy_mutual_info_classif \u001b[38;5;241m=\u001b[39m \u001b[43mperform_multiple_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m, in \u001b[0;36mperform_multiple_grid_search\u001b[1;34m(param_grid, X_train, y_train_encoded, n_runs)\u001b[0m\n\u001b[0;32m     19\u001b[0m grid_search_mutual_info_classif \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline_mutual_info_classif, param_grid, cv\u001b[38;5;241m=\u001b[39mcv, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Fit the grid search objects to the data\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mgrid_search_f_classif\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m grid_search_mutual_info_classif\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_encoded)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Get optimal k for each method\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1543\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:914\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    910\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    911\u001b[0m         )\n\u001b[0;32m    912\u001b[0m     )\n\u001b[1;32m--> 914\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def perform_multiple_grid_search(param_grid, X_train, y_train_encoded, n_runs=20):\n",
    "    best_ks_f_classif = []\n",
    "    best_ks_mutual_info_classif = []\n",
    "    \n",
    "    # Store accuracies for each k\n",
    "    accuracy_f_classif = {k: [] for k in param_grid['selector__k']}\n",
    "    accuracy_mutual_info_classif = {k: [] for k in param_grid['selector__k']}\n",
    "    \n",
    "    # Perform grid search over multiple runs\n",
    "    for _ in range(n_runs):\n",
    "        # Create cross-validation strategy\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
    "        \n",
    "        # Define grid search objects\n",
    "        grid_search_f_classif = GridSearchCV(pipeline_f_classif, param_grid, cv=cv, n_jobs=-1, verbose=0)\n",
    "        grid_search_mutual_info_classif = GridSearchCV(pipeline_mutual_info_classif, param_grid, cv=cv, n_jobs=-1, verbose=0)\n",
    "\n",
    "        # Fit the grid search objects to the data\n",
    "        grid_search_f_classif.fit(X_train, y_train_encoded)\n",
    "        grid_search_mutual_info_classif.fit(X_train, y_train_encoded)\n",
    "\n",
    "        # Get optimal k for each method\n",
    "        best_k_f_classif = grid_search_f_classif.best_params_['selector__k']\n",
    "        best_k_mutual_info_classif = grid_search_mutual_info_classif.best_params_['selector__k']\n",
    "        \n",
    "        best_ks_f_classif.append(best_k_f_classif)\n",
    "        best_ks_mutual_info_classif.append(best_k_mutual_info_classif)\n",
    "\n",
    "        # Record accuracies for each k\n",
    "        for mean, params in zip(grid_search_f_classif.cv_results_['mean_test_score'], grid_search_f_classif.cv_results_['params']):\n",
    "            accuracy_f_classif[params['selector__k']].append(mean)\n",
    "\n",
    "        for mean, params in zip(grid_search_mutual_info_classif.cv_results_['mean_test_score'], grid_search_mutual_info_classif.cv_results_['params']):\n",
    "            accuracy_mutual_info_classif[params['selector__k']].append(mean)\n",
    "    \n",
    "    # Calculate average optimal k and accuracy\n",
    "    avg_best_k_f_classif = np.mean(best_ks_f_classif)\n",
    "    avg_best_k_mutual_info_classif = np.mean(best_ks_mutual_info_classif)\n",
    "\n",
    "    print(f'Average optimal value of k for f_classif over {n_runs} runs: {avg_best_k_f_classif}')\n",
    "    print(f'Average optimal value of k for mutual_info_classif over {n_runs} runs: {avg_best_k_mutual_info_classif}')\n",
    "    \n",
    "    print(\"\\nAverage accuracy for each k (f_classif):\")\n",
    "    for k, accuracies in accuracy_f_classif.items():\n",
    "        print(f\"k = {k}: Average Accuracy = {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "    \n",
    "    print(\"\\nAverage accuracy for each k (mutual_info_classif):\")\n",
    "    for k, accuracies in accuracy_mutual_info_classif.items():\n",
    "        print(f\"k = {k}: Average Accuracy = {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "    print(f'\\n\\n best k_f {best_ks_f_classif}')\n",
    "    print(f'\\n\\n best mutual info {best_ks_mutual_info_classif}')\n",
    "    return avg_best_k_f_classif, avg_best_k_mutual_info_classif, accuracy_f_classif, accuracy_mutual_info_classif\n",
    "\n",
    "# Example usage:\n",
    "param_grid = {'selector__k': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}\n",
    "avg_best_k_f_classif, avg_best_k_mutual_info_classif, accuracy_f_classif, accuracy_mutual_info_classif = perform_multiple_grid_search(param_grid, X_train, y_train_encoded, n_runs=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the 30 runs, each candidate k value is selected an arbitrary number of times.\n",
    "\n",
    "Here, we use the array outputted with various k values and find the number of times each value it appears.\n",
    "\n",
    "Taking the mode of each list gives us the truly optimal k value.\n",
    "\n",
    "list1 is for f_classif, list2 is for mutual_info_classif criteria - the arrays with the 30 values returned in our trial are listed, but even they may be slightly different across trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in list1: 19: 8, 21: 5, 17: 4, 20: 4, 15: 3, 18: 2, 16: 2, 11: 1, 14: 1\n",
      "Counts in list2: 19: 9, 18: 5, 20: 5, 12: 2, 15: 2, 16: 2, 13: 1, 11: 1, 21: 1, 10: 1, 17: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get the union of keys from both counters\n",
    "\n",
    "best_ks_f_classif = [18, 11, 21, 17, 17, 20, 19, 19, 21, 14, 19, 20, 16, 19, 19, 19, 21, 20, 21, 15, 15, 20, 18, 21, 19, 17, 15, 17, 16, 19]\n",
    "best_ks_mutual_info_classif = [19, 12, 18, 15, 19, 18, 18, 20, 19, 20, 19, 16, 18, 20, 19, 16, 13, 15, 11, 19, 20, 21, 10, 18, 20, 19, 19, 19, 12, 17]\n",
    "counter1 = Counter(best_ks_f_classif)\n",
    "counter2 = Counter(best_ks_mutual_info_classif)\n",
    "\n",
    "# Format counts and sort by values (counts) in descending order\n",
    "formatted_counts1 = \", \".join(f\"{key}: {value}\" for key, value in sorted(counter1.items(), key=lambda x: x[1], reverse=True))\n",
    "formatted_counts2 = \", \".join(f\"{key}: {value}\" for key, value in sorted(counter2.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Print results\n",
    "print(f\"Counts in list1: {formatted_counts1}\")\n",
    "print(f\"Counts in list2: {formatted_counts2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal k-value (both) is 19.\n",
    "\n",
    "Evaluate using this on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for f_classif: 0.6608\n",
      "F1-score for f_classif: 0.6598\n",
      "Accuracy for mutual_info_classif: 0.9476\n",
      "F1-score for mutual_info_classif: 0.9475\n"
     ]
    }
   ],
   "source": [
    "# Fit with k=19 for both methods\n",
    "pipeline_f_classif.set_params(selector__k=19)\n",
    "\n",
    "pipeline_f_classif.fit(X_train, y_train_encoded)\n",
    "\n",
    "pipeline_mutual_info_classif.set_params(selector__k=19)\n",
    "\n",
    "pipeline_mutual_info_classif.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict the target values\n",
    "\n",
    "y_pred_f_classif = pipeline_f_classif.predict(X_test)\n",
    "\n",
    "y_pred_mutual_info_classif = pipeline_mutual_info_classif.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and f1-score\n",
    "\n",
    "accuracy_f_classif = accuracy_score(y_test_encoded, y_pred_f_classif)\n",
    "f1_f_classif = f1_score(y_test_encoded, y_pred_f_classif, average='weighted')\n",
    "\n",
    "accuracy_mutual_info_classif = accuracy_score(y_test_encoded, y_pred_mutual_info_classif)\n",
    "f1_mutual_info_classif = f1_score(y_test_encoded, y_pred_mutual_info_classif, average='weighted')\n",
    "\n",
    "print(f\"Accuracy for f_classif: {accuracy_f_classif:.4f}\")\n",
    "print(f\"F1-score for f_classif: {f1_f_classif:.4f}\")\n",
    "\n",
    "print(f\"Accuracy for mutual_info_classif: {accuracy_mutual_info_classif:.4f}\")\n",
    "print(f\"F1-score for mutual_info_classif: {f1_mutual_info_classif:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model with f_classif: 0.9265734265734266\n",
      "F1-score of the best model with f_classif: 0.9288135593220339\n",
      "Accuracy of the best model with mutual_info_classif: 0.9090909090909091\n",
      "F1-score of the best model with mutual_info_classif: 0.9115646258503401\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models obtained with the two pipelines on the testing dataset\n",
    "\n",
    "# get the best models from the grid search object\n",
    "best_model_f_classif = grid_search_f_classif.best_estimator_\n",
    "\n",
    "best_model_mutual_info_classif = grid_search_mutual_info_classif.best_estimator_\n",
    "\n",
    "# print the accuracy and f1-score of the best models on the testing dataset\n",
    "\n",
    "y_pred_f_classif = best_model_f_classif.predict(X_test)\n",
    "y_pred_mutual_info_classif = best_model_mutual_info_classif.predict(X_test)\n",
    "\n",
    "accuracy_f_classif = accuracy_score(y_test_encoded, y_pred_f_classif)\n",
    "f1_f_classif = f1_score(y_test_encoded, y_pred_f_classif)\n",
    "\n",
    "accuracy_mutual_info_classif = accuracy_score(y_test_encoded, y_pred_mutual_info_classif)\n",
    "f1_mutual_info_classif = f1_score(y_test_encoded, y_pred_mutual_info_classif)\n",
    "\n",
    "print(f'Accuracy of the best model with f_classif: {accuracy_f_classif}')\n",
    "print(f'F1-score of the best model with f_classif: {f1_f_classif}')\n",
    "\n",
    "print(f'Accuracy of the best model with mutual_info_classif: {accuracy_mutual_info_classif}')\n",
    "print(f'F1-score of the best model with mutual_info_classif: {f1_mutual_info_classif}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best feature selection method is with the criterion: mutual_info_classif\n",
    "\n",
    "The accuracy and f1-scores in this case are slightly better, but both models are pretty much the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dropped feature is: ['absences']\n"
     ]
    }
   ],
   "source": [
    "# Check which features are actually selected\n",
    "# We have 21, and k=20, so one has been dropped\n",
    "\n",
    "all_features = ['hrs', 'absences', 'JobInvolvement', 'PerformanceRating',\n",
    "       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age',\n",
    "       'DistanceFromHome', 'Education', 'EmployeeID', 'JobLevel',\n",
    "       'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
    "       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
    "# get the selected features from the best model with mutual_info_classif\n",
    "selected_features_mutual_info_classif = best_model_mutual_info_classif.named_steps['selector'].get_support()\n",
    "\n",
    "# find the feature that has been dropped\n",
    "dropped_feature = [feature for feature, selected in zip(all_features, selected_features_mutual_info_classif) if not selected] # zip is used to iterate over two lists at the same time\n",
    "print(f'The dropped feature is: {dropped_feature}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features, besides 'absences' are selected. 20 total features\n",
    "\n",
    "The results are improved as compared to the previous assignment, F1 Score has increased from 0.9320 to 0.9424\n",
    "\n",
    "Reason is likely that 'absences' might have been so poorly correlated to attrition that it contributed noise, which means the model can improve its predictions after removing it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for the best model with mutual_info_classif: {'classifier__max_depth': 10, 'classifier__n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# Redefine the pipeline with k = 20\n",
    "\n",
    "pipeline_mutual_info_classif = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest(mutual_info_classif, k=20)), # add this feature selection\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Do hyper-parameter tuning with the new pipeline\n",
    "\n",
    "# define the parameters for the grid search\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'classifier__max_depth': [5, 10, 15, 20, 25]\n",
    "}\n",
    "\n",
    "# perform grid search\n",
    "grid_search_mutual_info_classif = GridSearchCV(pipeline_mutual_info_classif, param_grid, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "grid_search_mutual_info_classif.fit(X_train, y_train_encoded)\n",
    "\n",
    "# print the optimal parameters\n",
    "\n",
    "best_params_mutual_info_classif = grid_search_mutual_info_classif.best_params_\n",
    "\n",
    "print(f'Optimal parameters for the best model with mutual_info_classif: {best_params_mutual_info_classif}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate whether results are further improved\n",
    "\n",
    "As usual, we are using accuracy as the metric to evaluate the model, because it is a classification problem and the classes are balanced\n",
    "\n",
    "But also, we add the F1-score to have a more complete view of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model with mutual_info_classif after HPO: 0.9545454545454546\n",
      "F1-score of the best model with mutual_info_classif after HPO: 0.9559322033898305\n"
     ]
    }
   ],
   "source": [
    "# Check if results are further improved\n",
    "\n",
    "# get the best model from the grid search object\n",
    "best_model_mutual_info_classif = grid_search_mutual_info_classif.best_estimator_\n",
    "\n",
    "# predict the target values\n",
    "y_pred_mutual_info_classif = best_model_mutual_info_classif.predict(X_test)\n",
    "\n",
    "# calculate accuracy and f1-score\n",
    "accuracy_mutual_info_classif = accuracy_score(y_test_encoded, y_pred_mutual_info_classif)\n",
    "f1_mutual_info_classif = f1_score(y_test_encoded, y_pred_mutual_info_classif)\n",
    "\n",
    "print(f'Accuracy of the best model with mutual_info_classif after HPO: {accuracy_mutual_info_classif}')\n",
    "print(f'F1-score of the best model with mutual_info_classif after HPO: {f1_mutual_info_classif}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are further improved, mostly because we eliminated the redundant feature ('absences').\n",
    "\n",
    "  * This reduces the number of dimensions which reduces the risk of overfitting, so the model can perform better on previously unseen data\n",
    "\n",
    "  * With a more reliable (smaller) set of features, the use of 5-fold cross-validation ensures that there are multiple subsets of data that are being trained and tested on a more relevant model\n",
    "\n",
    "  * The number of trees is very high, as HPO chooses but this doesn't cause overfitting because the growth of each of those many trees is controlled (max_depth = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went back and saved the competition_data (already preprocessed) in a pickle file from Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./final_model_14.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on competition data\n",
    "\n",
    "# load the competition data\n",
    "\n",
    "X_competition = joblib.load('./competition_data.pkl')\n",
    "\n",
    "# predict the target values for the competition data using the best model with mutual_info_classif after HPO\n",
    "\n",
    "y_pred_competition = best_model_mutual_info_classif.predict(X_competition)\n",
    "\n",
    "# save the predictions to a file\n",
    "\n",
    "joblib.dump(y_pred_competition, './attrition_predictions_14.csv')\n",
    "\n",
    "# save the model\n",
    "\n",
    "joblib.dump(best_model_mutual_info_classif, './final_model_14.pkl')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
