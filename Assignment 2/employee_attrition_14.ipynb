{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Feature Selection for Attrition / Burnout Prediction\n",
    "\n",
    "# Group 14: Rylie Ramos-Marquez, Derek Atabayev, Vishnu Garigipati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous assignment, we know that the mest method is gradient boosting of trees, specifically with 200 boosting rounds.\n",
    "\n",
    "Why it's the best:\n",
    "\n",
    "* High F1-score = 0.9319 which far surpasses other models (better by at least 0.1 = 10%), and is also better than simple KNN/Decision trees model\n",
    "\n",
    "* Shallow tree structure (max depth = 5) which prevents overfitting\n",
    "\n",
    "* Consistent performance with a tight 95% confidence interval of [0.855171, 0.893952] which is a very small range, meaning the performance is consistent and strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model is saved as a Pickle format for easy retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import joblib # since joblib.dump was used to save the model\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('scaler',\n",
      "                                                                   MinMaxScaler())]),\n",
      "                                                  Index(['hrs', 'absences', 'JobInvolvement', 'PerformanceRating',\n",
      "       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age',\n",
      "       'DistanceFromHome', 'Education', 'EmployeeID', 'JobLevel',\n",
      "       'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
      "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
      "       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier',\n",
      "                 GradientBoostingClassifier(max_depth=5, min_samples_split=5,\n",
      "                                            n_estimators=200,\n",
      "                                            random_state=100545358,\n",
      "                                            subsample=0.8))])\n"
     ]
    }
   ],
   "source": [
    "# retrieve final_model_14.pkl from ../final_model_14.pkl\n",
    "\n",
    "model = joblib.load('../final_model_14.pkl')\n",
    "\n",
    "# check if the model has been loaded correctly\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model has been loaded successfully. We can see that the feature names are correct and familiar, so the preprocessing and classifier are intact. All the gradient boosting parameters are there too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the grid search later, we will need our training data. We can pull that now too.\n",
    "\n",
    "We added a cell to the notebook from assignment 1 to export this data to pickle files\n",
    "\n",
    "Our ideal split from assignment one was 80/20 training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data X and y from pickle\n",
    "\n",
    "X = joblib.load('./X.pkl')\n",
    "y = joblib.load('./y.pkl')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100545358)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SelectKBest and criterion f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add feature selection with SelectKBest and f_classif to the pipeline\n",
    "\n",
    "# score_func is the function used to evaluate the importance of each feature\n",
    "# f_classif is a default value for the score_func parameter, calculates ANOVA F-Value\n",
    "\n",
    "# Extract components from the existing pipeline\n",
    "preprocessor = model.named_steps['preprocessor']\n",
    "classifier = model.named_steps['classifier']\n",
    "\n",
    "# create pipeline 1: feature selection with SelectKBest and criterion f_classif\n",
    "\n",
    "pipeline_f_classif = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest(f_classif, k=10)),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "    \n",
    "\n",
    "# create pipline 2: feature selection with SelectKBest and criterion mutual_info_classif\n",
    "\n",
    "pipeline_mutual_info_classif = Pipeline([\n",
    "    ('model', model),\n",
    "    ('selector', SelectKBest(mutual_info_classif, k=10)),\n",
    "    ('classifier', classifier)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing grid search to tune the number of features to be selected (k parameter)\n",
    "\n",
    "  * We need to use an array to test different values of k, since we have 21 features, we can check 5, 10, 15, 20, 21 possibilities\n",
    "  * This will tell us whether the higher side or lower side is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 25 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n25 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 472, in fit\n    Xt = self._fit(X, y, routed_params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 389, in _fit\n    self._validate_steps()\n  File \"c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 259, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  Index(['hrs', 'absences', 'JobInvolvement', 'PerformanceRating',\n       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age',\n       'DistanceFromHome', 'Education', 'EmployeeID', 'JobLevel',\n       'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n      dtype='object'))])),\n                ('classifier',\n                 GradientBoostingClassifier(max_depth=5, min_samples_split=5,\n                                            n_estimators=200,\n                                            random_state=100545358,\n                                            subsample=0.8))])' (type <class 'sklearn.pipeline.Pipeline'>) doesn't\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grid_search_f_classif, grid_search_mutual_info_classif\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Call the function with the parameter grid\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m grid_search_f_classif, grid_search_mutual_info_classif \u001b[38;5;241m=\u001b[39m \u001b[43mperform_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mperform_grid_search\u001b[1;34m(param_grid)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# fit the grid search objects to the data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m grid_search_f_classif\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_encoded)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mgrid_search_mutual_info_classif\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print the optimal k in both cases\u001b[39;00m\n\u001b[0;32m     24\u001b[0m best_k_f_classif \u001b[38;5;241m=\u001b[39m grid_search_f_classif\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselector__k\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1543\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:945\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    940\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    943\u001b[0m     )\n\u001b[1;32m--> 945\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 25 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n25 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 472, in fit\n    Xt = self._fit(X, y, routed_params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 389, in _fit\n    self._validate_steps()\n  File \"c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 259, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  Index(['hrs', 'absences', 'JobInvolvement', 'PerformanceRating',\n       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age',\n       'DistanceFromHome', 'Education', 'EmployeeID', 'JobLevel',\n       'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n      dtype='object'))])),\n                ('classifier',\n                 GradientBoostingClassifier(max_depth=5, min_samples_split=5,\n                                            n_estimators=200,\n                                            random_state=100545358,\n                                            subsample=0.8))])' (type <class 'sklearn.pipeline.Pipeline'>) doesn't\n"
     ]
    }
   ],
   "source": [
    "# performing grid search to find the best parameters for the model, best metric is accuracy\n",
    "\n",
    "# define the parameters for the grid search\n",
    "\n",
    "# parameters for choosing a value of k\n",
    "param_grid = {\n",
    "    'selector__k': [5,10,15,20,21]\n",
    "}\n",
    "\n",
    "def perform_grid_search(param_grid):\n",
    "    # create grid search object for pipeline 1\n",
    "    grid_search_f_classif = GridSearchCV(pipeline_f_classif, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "    # in the previous assignment, we used 5-fold cross validation because the dataset is small, and we want to make sure that the model is not overfitting\n",
    "    # that approach worked well and so we can use it for HPO of the 'k' parameter too\n",
    "\n",
    "    # creating grid search object for pipeline 2\n",
    "    grid_search_mutual_info_classif = GridSearchCV(pipeline_mutual_info_classif, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "    # fit the grid search objects to the data\n",
    "    grid_search_f_classif.fit(X_train, y_train_encoded)\n",
    "    grid_search_mutual_info_classif.fit(X_train, y_train_encoded)\n",
    "\n",
    "    # print the optimal k in both cases\n",
    "    best_k_f_classif = grid_search_f_classif.best_params_['selector__k']\n",
    "    print(f'Optimal value of k for f_classif: {best_k_f_classif}')\n",
    "\n",
    "    best_k_mutual_info_classif = grid_search_mutual_info_classif.best_params_['selector__k']\n",
    "    print(f'Optimal value of k for mutual_info_classif: {best_k_mutual_info_classif}')\n",
    "\n",
    "    return grid_search_f_classif, grid_search_mutual_info_classif\n",
    "\n",
    "# Call the function with the parameter grid\n",
    "grid_search_f_classif, grid_search_mutual_info_classif = perform_grid_search(param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the best k value is 20 in both cases; only one k-value is being discarded for minimal predictive power.\n",
    "\n",
    "This suggests that overall, the features havevery strong predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model with f_classif: 0.9195804195804196\n",
      "F1-score of the best model with f_classif: 0.9225589225589226\n",
      "Accuracy of the best model with mutual_info_classif: 0.9300699300699301\n",
      "F1-score of the best model with mutual_info_classif: 0.9319727891156463\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models obtained with the two pipelines on the testing dataset\n",
    "\n",
    "# get the best models from the grid search object\n",
    "best_model_f_classif = grid_search_f_classif.best_estimator_\n",
    "\n",
    "best_model_mutual_info_classif = grid_search_mutual_info_classif.best_estimator_\n",
    "\n",
    "# print the accuracy and f1-score of the best models on the testing dataset\n",
    "\n",
    "y_pred_f_classif = best_model_f_classif.predict(X_test)\n",
    "y_pred_mutual_info_classif = best_model_mutual_info_classif.predict(X_test)\n",
    "\n",
    "accuracy_f_classif = accuracy_score(y_test_encoded, y_pred_f_classif)\n",
    "f1_f_classif = f1_score(y_test_encoded, y_pred_f_classif)\n",
    "\n",
    "accuracy_mutual_info_classif = accuracy_score(y_test_encoded, y_pred_mutual_info_classif)\n",
    "f1_mutual_info_classif = f1_score(y_test_encoded, y_pred_mutual_info_classif)\n",
    "\n",
    "print(f'Accuracy of the best model with f_classif: {accuracy_f_classif}')\n",
    "print(f'F1-score of the best model with f_classif: {f1_f_classif}')\n",
    "\n",
    "print(f'Accuracy of the best model with mutual_info_classif: {accuracy_mutual_info_classif}')\n",
    "print(f'F1-score of the best model with mutual_info_classif: {f1_mutual_info_classif}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best feature selection method is with the criterion: mutual_info_classif\n",
    "\n",
    "The accuracy and f1-scores in this case are slightly better, but both models are pretty much the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dropped feature is: []\n"
     ]
    }
   ],
   "source": [
    "# Check which features are actually selected\n",
    "# We have 21, and k=20, so one has been dropped\n",
    "\n",
    "all_features = ['hrs', 'absences', 'JobInvolvement', 'PerformanceRating',\n",
    "       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age',\n",
    "       'DistanceFromHome', 'Education', 'EmployeeID', 'JobLevel',\n",
    "       'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
    "       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
    "# get the selected features from the best model with mutual_info_classif\n",
    "selected_features_mutual_info_classif = best_model_mutual_info_classif.named_steps['selector'].get_support()\n",
    "\n",
    "# find the feature that has been dropped\n",
    "dropped_feature = [feature for feature, selected in zip(all_features, selected_features_mutual_info_classif) if not selected] # zip is used to iterate over two lists at the same time\n",
    "print(f'The dropped feature is: {dropped_feature}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features, besides 'absences' are selected. 20 total features\n",
    "\n",
    "The results are improved as compared to the previous assignment, F1 Score has increased from 0.9320 to 0.9424\n",
    "\n",
    "Reason is likely that 'absences' might have been so poorly correlated to attrition that it contributed noise, which means the model can improve its predictions after removing it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the pipeline with k = 20\n",
    "\n",
    "pipeline_mutual_info_classif = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest(mutual_info_classif, k=20)), # add this feature selection\n",
    "    ('classifier', classifier)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
